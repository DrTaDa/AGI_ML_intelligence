# The non-mystery of consciousness: the theory of Juergen Schmidhuber - 12/05/2020

When trying to define consciousness, we quickly get lost in the diversity of phenomena it surfaces through. Browsing definitions of consciousness proposed by philosophers, neuroscientists or psychologists feels like each holds a part of the truth without any ever being able to grasp its essence. Almost as if the word consciousness could only be polysemic.

However, that would be forgetting that most of the natural phenomena we consider today as trivial went through a similar phase of vagueness. Think about the theory of evolution. Before Darwinism, people noticed the diversity among species, they collected and categorized fossils, farmers performed selective breeding and domestication for generations, but nobody could pinpoint exactly what was the common thread. Until one day, as if enough pieces of the puzzle had been gathered, the underlying picture had shown through and a coherent theory could be proposed.

If I wanted to write this article, it is because I am convinced that for consciousness, that step already happened. And the man behind the elegant theory that seems to perfectly encapsulate consciousness, is Juergen Schmidhuber, a researcher at the Dalle Molle institute for artificial intelligence in Switzerland.

Let's start from a simple thought experiment from the field of artificial intelligence (A.I.), as Pr. Schmidhuber does. It might seem counterintuitive to start from today's A.I. systems as they seem vastly inferior to any biological brain, especially in the context of consciousness. But as we saw above, trying to understand consciousness by looking directly at the fractal complexity of the human brain is overwhelming. Starting from a simpler system, as it is often done when studying a new phenomenon in physics and mathematics, is useful to develop an initial intuition from which one can then generalize.

Picture a robotic arm controlled by an artificial neural network, or more precisely, by a recurrent neural network (R.N.N.). This arm is trained to perform the task of moving a cube from one end of a table to the other. A camera is located above the table and it's video feed is used as a sensory input by the R.N.N. For learning to happen, the network needs to create an internal representation of the visual elements useful to the task such as the cube or the table. In artificial neural networks, that happens through a compression process. The network compresses the input data by reducing it's dimensionality which allows to smooth out the noise but also to produce representations for the objects important for the task in a simpler space. Both of these leading to generalization and learning. The final product of this learning process is referred to as a latent space, in which objects or states are represented as vectors. For example, in our thought experiment, the learning process might take place on different tables having different shapes and color. Since these tables always play the same role in the task at hand, we expect them to be represented by almost collinear vectors in the latent space. On the other hand, the cube plays a very different role with respect to the task, and therefore will be represented by a vector highly orthogonal to the representation of any table.

Now let's introduce a second R.N.N. The task of this second network is not to control the arm, but to predict what the results of the arm's actions might be. As for our first network, this second one has to create representations of the visual elements important to predict what's going on on the table. And this is where something interesting happens. There is one thing that is always present on-camera and that is crucially important when trying to understand what is happening on the table - the arm itself. The network needs an internal representation of the arm as a physical entity composed of color, texture, shape, but more than that, it needs a model of how the arm interacts with and affects the world. And so the network learns a representation, a model, of its physical self and how "it" can affect the world.

But we can go further, let's say that in addition to the camera feed, this second network can also see the state of the first network, the one in charge of controlling the arm. As the state of this first network is directly correlated to the motion of the arm, it is an input of prime importance for the second network to predict what's going on on the table. Therefore, this second network also creates a representation of how the state of the first network influences the world. And that, is consciousness: a mental model of your physical and mental self and of how they can interact with the world.

It might seem rushed to associate this model of self, developed by a simple A.I., to consciousness. But let me show you two properties of the above thought experiment that will make this gap seem smaller. The first convincing property is that the driving factor leading to the emergence of this mental model is usefulness. The A.I. develops the representation of the self as a necessity in order to accomplish its task more efficiently. It might sound obvious but it is an important property because nature values usefulness and always favors efficient problem solvers that minimize energy expenditure. Therefore, if a simple N.N. developed a model of self to perform better, it would be likely for a similar phenomenon to appear in animals through natural selection. A second convenient property of the present theory, is that it generalizes to any level of consciousness. If consciousness appeared through an evolutionary process, at first, it must have done so in a simple form. As of today, it would be inelegant to design a theory of consciousness that would work perfectly in the case of humans but not for the ant or the dolphin. And the above thought experiment shows that mental models of self can appear in very simple circumstances. Looking at these properties, it seems there isn't much missing to our A.I. to call what it had to develop consciousness.

If we accept to take that step, the question is, what does that say about us humans? Following our newly accepted definition of consciousness, we now know that to understand what human's consciousness is, we need to know how humans interact with the world. The question becomes: what possibilities of interaction are offered to humans by their physical and mental abilities? Opposable thumbs, upright posture and oversized neo-cortex or some of the characteristics that we use to draw humans apart from the rest of the animal realm. Just look at the human hand and admire the unreasonable precision with which it can be used to shape our environment. The myriad of possibilities that hands offered needed an internal representation for humans to exploit them to the best of their abilities, and therefore have a higher survival rate. Therefore, co-evolution had to take place between the human body and the mental representations needed to make the best use of it. This co-evolution process intensified when we started developing tools which multiplied hundred folds the conceivable interactions we have with the world. If I do some introspection, I quickly notice that I internalized the use of glasses, computers or cars when I think about myself or how I plan to interact with the world. In addition to our physical self and our material interaction with the world, this demonstration can be extended to social interactions. On a day to day basis, what most of us do as men and women, is not so much to take actions on the physical world, but rather to interact with one another and influence each other's psyche. To do so successfully, we also needed to develop mental models of how others perceive us in social situations and of the social tools available to us when interacting with one another. To summarize, consciousness is a side-effect of the embodiment of the human experience, but the tools we use also influence our consciousness on a fundamental level.

---

When an interviewer asked Juergen Schmidhuber if we will one day design a conscious A.I., his answer was resolute: it already happened. Because any intelligent system, organic or numeric, that needed a representation of itself to solve its task had, at least in an elementary form, developed consciousness.
Now, there is one question left. These embryos of a consciousness seem to us humans completely disconnected from the mystified concept of consciousness we surround ourselves with. But as we will produce more and more advanced A.I. systems, when will we be able to say "that thing is conscious"?
And to accept that they are indeed conscious will be left to us, as a leap of faith.

---

To go further, read [Driven by Compression Progress, Jurgen Schmidhuber](https://arxiv.org/pdf/0812.4360.pdf).
